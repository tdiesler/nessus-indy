{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data sets\n",
    "\n",
    "There are many times when you need to combine data from different sources. Besides having a need to combine data, how you want to combine it will vary and that's why there are so many options available. \n",
    " \n",
    "For example:\n",
    " \n",
    "- you may separate files for each month and you want to combine them together so you can analyse the entire year. Since the structure/columns are the same, you would simply want to stack these on top of one another.\n",
    " \n",
    "- On the other hand, you may have cases where you have different types of tables that you want to combine horizontally on the records so that you can compare -- for example, joining an employee master list to payroll list to confirm that all the employees are being paid the appropriate amounts each month.\n",
    " \n",
    "In cases like these, you would need to combine the Pandas series and dataframe once you have read the data from the original data sources into Pandas. Pandas provides functions to combine data in Pandas objects.\n",
    " \n",
    "Let’s explore these functions now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many a times we have to combine data from different sources into one, for e.g.:<br>\n",
    "- Reading data from multiple files, and combining (append,merge,join) them together to form one dataset\n",
    "- Joining data from two database tables etc</br>\n",
    "\n",
    "In this case, once we have read the data from the original datasources into Pandas, we would need to combine together the Pandas Series and DataFrames\n",
    "\n",
    "Pandas provide various built in functions to combine together the data contained in Pandas objects, and this section we will explore these functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of Data using concat()\n",
    "\n",
    "\n",
    "Concatenation is a common operation that combines two datasets. It is also referred to as ‘stacking’ or ‘binding’ data together.\n",
    " \n",
    "It’s better to understand the operation with an example. Look at this diagram:\n",
    "\n",
    "![Concatenation](dataframes/Python-concat-example-3df.png)\n",
    " \n",
    "As you can see, there are three dataframes on the left and they have been concatenated (or stacked together) to produce the dataframe on the right.\n",
    " \n",
    "Concatenation follows certain principles:\n",
    " \n",
    "- The operation happens along a particular axis.\n",
    "- While performing the operation along that axis, you can apply some set logic operations (union or intersection) on the other axes.\n",
    " \n",
    "As a result, there are some variations for how you can perform concatenation. \n",
    "Let's use some code to implement the simple concatenation operation depicted above. We have adopted a very basic approach:\n",
    " \n",
    "- Create the three dataframes.\n",
    "- Concatenate the three dataframes with the concat() Pandas function. \n",
    "- Use a list of dataframes as input for the function.\n",
    " \n",
    "Use these code snippets for a demonstration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Define DataFrame 1\n",
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                   index=[0, 1, 2, 3])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define DataFrame #2\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                    'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                    'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "                    'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "                   index=[4, 5, 6, 7])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define DataFrame #3\n",
    "df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n",
    "                    'B': ['B8', 'B9', 'B10', 'B11'],\n",
    "                    'C': ['C8', 'C9', 'C10', 'C11'],\n",
    "                    'D': ['D8', 'D9', 'D10', 'D11']},\n",
    "                   index=[8, 9, 10, 11])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the concatenation of the three DataFrames\n",
    "result = pd.concat([df1,df2,df3])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the syntax for concat() where the function accepts various parameters depending on what you want to perform on the DataFrame : \n",
    " \n",
    "<CODE>\n",
    "pandas.concat(objs: Union[Iterable[‘DataFrame’], Mapping[Label, ‘DataFrame’]], axis='0', join: str = \"'outer'\", ignore_index: bool = 'False', keys='None', levels='None', names='None', verify_integrity: bool = 'False', sort: bool = 'False', copy: bool = 'True') → ’DataFrame’[source]\n",
    "</CODE> \n",
    "\n",
    "Apply these parameters for more control over the operation. \n",
    "\n",
    "\n",
    "- __objs__ : This parameter defines a sequence, mapping of series, or dataframe objects.\n",
    "\n",
    "- __axis__ : This defines the axis to concatenate along. The options are {0, 1, …}, with the default as 0.\n",
    "\n",
    "- __join__ : This defines how the operation should handle indexes on other axes. The options are {‘inner’, ‘outer’}, with the default as ‘outer’. Use ‘outer’ for union and ‘inner’ for intersection.\n",
    "\n",
    "- __ignore_index__ : This is useful if you are concatenating objects when the concatenation axis doesn’t have meaningful indexing information.\n",
    "\n",
    "- __keys__ : This parameter is used to construct a hierarchical index, using the passed keys as the outermost level. If multiple levels are passed, the parameter should contain tuples. The value should be a sequence, with a default of None.\n",
    "\n",
    "- __levels__ : This parameter specifies levels (unique values) to use for constructing a MultiIndex. If you do not provide these values, they will be inferred from the keys. The values should be a list of sequences, with None as a default.\n",
    "\n",
    "-  __name__ : This parameter provides names for the levels in the resulting hierarchical index. The values are in list, with None as a default.\n",
    "\n",
    "- __verify_integrity__ : This checks if the new, concatenated axis contains duplicates. This operation can be very expensive relative to the actual data concatenation. The parameter is a Boolean value and is False by default.\n",
    "\n",
    "- __copy__ : This parameter is a Boolean value, with a default value of True. If it is set to False, the operation will not copy data unnecessarily.\n",
    "\n",
    "Check the reference guide for more information about these parameters. Please refer to the following link:<br>\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "\n",
    "We will now explore some of these parameters by way of examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1. Hierarchical Indexing using _\"keys\"_ parameter\n",
    "\n",
    "Let’s look at some examples of how to use the parameters.\n",
    "\n",
    "Refer back to the previous example with code snippets. If you are concatenating the original data frames, you might want to retain the original keys (0, 1, 2, etc) and indicate which source (dataframe) the data comes from. \n",
    " \n",
    "You can achieve this with hierarchical indexing, where you add a top-level key that identifies the original dataframe. In this case, you would use the key parameter of the function and pass the list of keys that corresponds to the sequence of dataframe being concatenated.\n",
    " \n",
    "Use this code for an example to reuse the previously defined DataFrames df1, df2 and df3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reuse the previously defined DataFrames df1, df2 and df3\n",
    "res = pd.concat([df1,df2,df3], keys=['a','b','c'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a dataframe with hierarchical indexing in place. You can access the chunk of this dataframe by using the first-level key. \n",
    "\n",
    " \n",
    "Use this code for a demonstration: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.loc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.loc['c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the top-level key a, you can access the original data chunk from dataframe df1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Set logic on other axes using axis and join\n",
    "\n",
    "If you want a refresher on join operations in Set Theory, or outer join and inner joins, please refer to Basic SQL Join Types. You will also learn more about these concepts in Module 6.\n",
    " \n",
    "Go to: SQL Set Theory [2]\n",
    " \n",
    "You can apply set logic on the other axes by using the join parameter: \n",
    " \n",
    "join=outer: take the union of all. This is the default option (it also by default with sort the other axes)\n",
    "join=inner: take the intersection\n",
    " \n",
    "This parameter allows you to perform useful operations. Consider a situation where you need to concatenate on the column axis and perform an outer join (ie, union):\n",
    "\n",
    "![\"Outer Join\"](dataframes/Python-concat-example-outer.png)\n",
    " \n",
    "Notice that:\n",
    "\n",
    "\n",
    "the column labels from both axes have been concatenated\n",
    "a union operation has been performed for the row labels.\n",
    " \n",
    "How do you achieve this? Try this code snippet to create another DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create another DataFrame\n",
    "df4 = pd.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'],\n",
    "                    'D': ['D2', 'D3', 'D6', 'D7'],\n",
    "                    'F': ['F2', 'F3', 'F6', 'F7']},\n",
    "                   index=[2, 3, 6, 7])\n",
    "\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Default behaviour is join=outer, so we are not specifying the parameter explicitly\n",
    "result = pd.concat([df1,df4], axis=1, sort=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider a situation where you need to concatenate on the column axis and perform an inner join (ie, intersection):\n",
    "\n",
    "![\"Inner Join\"](dataframes/Python-concat-example-inner.png)\n",
    " \n",
    "Notice that:\n",
    "\n",
    "- the columns labels from both axes have been concatenated\n",
    "- an intersection operation has been performed for the row labels (ie, only the common rows from df1 and df4 are in the result). \n",
    " \n",
    "You can do the operation with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1,df4], axis=1, sort=False, join='inner')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to perform an outer join, but retain only the indexing from df1. You can do this by reindexing after concatenation. For the example above, the resulting DataFrame wouldn’t have rows 6 and 7. \n",
    " \n",
    "This diagram illustrates the result:\n",
    " \n",
    "![Outer Join and Reindexing](dataframes/Python-concat-example-outer-reindex.png)\n",
    " \n",
    "You can use this code to perform the operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check DF1\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check DF4\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1,df4], axis=1).reindex(df1.index)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have also reindexed df4, before the concatenation and would have got the same result. See below for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1,df4.reindex(df1.index)], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Ignoring Indexes on Concatenation Axes\n",
    "\n",
    "You might have a scenario where you don't have a meaningful index, or there is an overlapping index. In that case, you can still append the dataframe and ignore the index. The operation will result in a dataframe with a new index. To do this, use the ignore_index parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1,df4], ignore_index=True, sort=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of Data using append()\n",
    "\n",
    "__append()__ is the instance method available for dataframe and Series, and can be used to perform the concatenation operation. This method concatenates along the index (ie, axis=0).\n",
    " \n",
    "The syntax for append() is: \n",
    " \n",
    "<CODE>\n",
    "DataFrame.append(other, ignore_index=False, verify_integrity=False, sort=False)\n",
    "</CODE> \n",
    "\n",
    "These diagrams and code snippets illustrate how to use the method:\n",
    " \n",
    "![Append](dataframes/Python-concat-example-append.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df1.append(df2)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let us write a code for the following view of your DataFrame.\n",
    "\n",
    "![Append Disjoint](dataframes/Python-concat-example-append-disjoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df1.append(df4, sort=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more possibilities and variations possible for concat() and append() that are beyond the scope of this module. If you would like to explore further, please refer to this guide. \n",
    "\n",
    "If you would like to explore further please refer to the following link: <br>\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging data sets\n",
    "\n",
    "Instead of simply concatenating two data sets, sometimes you might need to merge them. What’s the difference? \n",
    " \n",
    "A concatenation means combining dataframes as additional rows or columns, regardless of the data values. However, suppose you need to integrate two separate dataframes where one contains the names of books and other contains the authors of those books; then merging is a more powerful and flexible function to rely on. Using this function, combine categories or indices that are common to both dataframes. \n",
    " \n",
    "Pandas has full-featured, high-performance, in-memory join operations, which are syntactically very similar to relational databases like SQL. Pandas provides a single function, merge(), as the entry point to all the standard database join operations between the dataframe objects or names series objects.\n",
    " \n",
    "The syntax of __merge()__ is: \n",
    " \n",
    "<CODE>\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=('x', 'y'), copy=True, indicator=False, validate=None)\n",
    "</CODE> \n",
    "\n",
    "\n",
    "Parameters for the merge() function\n",
    " \n",
    "- left: This defines the left-hand side set (dataframe or series object).\n",
    " \n",
    "- right: This defines the right-hand side set (dataframe or series object).\n",
    " \n",
    "- on=None : This defines the key (column name or index label) to join on. This key must be in both the right and left sets. The default value of this parameter is None. If this variable is not passed and both left_index and right_index are False, the operation infers the key for the join operation as the column intersection in the two dataframes.\n",
    " \n",
    "- left_on : These are the columns or index labels from the left set (dataframe or series) to use as keys.\n",
    " \n",
    "- right_on : These are the columns or index labels from the right set (dataframe or series) to use as keys.\n",
    " \n",
    "- left_index : If this Boolean value is True, use the index (row labels) from the left-hand set as the join keys.\n",
    " \n",
    "- right_index: If this Boolean value is True, use the index (row labels) from the right-hand set as join keys.\n",
    " \n",
    "- how: The possible values are 'left', 'right', 'outer', and 'inner' (the default). As the name suggests, it relates to left, right, outer, and inner joins.\n",
    " \n",
    "Refer to this documentation for a detailed list of merge() parameters and their uses. <br>\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore some examples.\n",
    "\n",
    "### Example 1: Inner Join(s)\n",
    "\n",
    "Look at these dataframes: \n",
    " \n",
    "![Inner Join](dataframes/Python-merge-inner.png)\n",
    "\n",
    "\n",
    "Notice that both the left and right dataframes have the same key columns with the same values. If you do an inner join on the key column, you will get a Dataframe that has columns A and B from the left dataframe, and columns C and D from the right dataframe.\n",
    " \n",
    "Try this code snippet in your Jupyter Notebook to see the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrames\n",
    "left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                     'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "                      'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                      'D': ['D0', 'D1', 'D2', 'D3']})\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge DataFrame, perform Inner Join\n",
    "pd.merge(left,right, on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Inner Join with Multi Keys\n",
    "\n",
    "Take a look at these dataframes:\n",
    "\n",
    "![Inner Join with Multi Keys](dataframes/Python-merge-inner-multi-key.png)\n",
    " \n",
    "Notice that:\n",
    " \n",
    "- Both original dataframes have columns 'key1' and 'key2'. If we perform an inner join on the two keys, the operation will use the combination of keys. \n",
    "- The key pair (K0, K0) occurs in both original dataframes once, so it will be in the resulting dataframe once and the columns will be merged.\n",
    "- The key pairs (K0, K1 and K2, K1) are not in the right-hand dataframe, so they won’t be in the resulting dataframe.\n",
    "- Key pair (K1, K0), from the left-hand dataframe, occurs twice in the right-hand dataframe, so we will get two rows with this key in the resulting dataframe.\n",
    " \n",
    "Try out the process with this code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n",
    "                   'key2': ['K0', 'K1', 'K0', 'K1'],\n",
    "                   'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                   'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "\n",
    "right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "                   'key2': ['K0', 'K0', 'K0', 'K0'],\n",
    "                   'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                   'D': ['D0', 'D1', 'D2', 'D3']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left,right, on=['key1','key2'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The how parameter determines how the join operation occurs (ie, which keys to include in the resulting table). If a key combination does not appear in either the left or right tables, the values in the joined table will be NA.\n",
    "\n",
    "Following table summarizes how the join operation works:\n",
    "\n",
    "| Merge Method | SQL Join Name | Description |\n",
    "|--------------|---------------|-------------|\n",
    "|left| LEFT OUTER JOIN | Use Keys from the LEFT Frame only|\n",
    "|right| RIGHT OUTER JOIN | Use Keys from the RIGHT Frame only|\n",
    "|outer| FULL OUTER JOIN | Use Union of Keys from both Frames |\n",
    "|inner| LEFT OUTER JOIN | Use Intersection of Keys from both Frames|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Left Join\n",
    "\n",
    "This diagram illustrates a left join:\n",
    "\n",
    "![Left Join](dataframes/Python-merge-left-join.png)\n",
    "\n",
    "Notice that:\n",
    "- all the key pairs from the left-hand dataframe are available \n",
    "- there are NA values in columns C and D if the key pair is not available in the right-hand dataframe\n",
    "- the key pair (K1, K0) appears twice, because it appears twice in the right-hand dataframe (with different values in   columns C and D).\n",
    "\n",
    "Let's use code to implement this join:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.merge(left,right, how='left', on=['key1','key2'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Right Join\n",
    "\n",
    "Here is an example of a right join:\n",
    " \n",
    "![Right Join](dataframes/Python-merge-right-join.png)\n",
    " \n",
    "As you can see, all the key pairs from the right-hand dataframe are available. Columns A and B have NA values where the key pair is not available in the right-hand dataframe.\n",
    " \n",
    "Here is some code to implement the right join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.merge(left,right, how='right', on=['key1','key2'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Outer Join\n",
    "\n",
    "Look at this diagram:\n",
    " \n",
    "![Outer Join](dataframes/Python-merge-outer.png)\n",
    "\n",
    "Notice that all the key pairs from the left- as well as right-hand dataframe are present in the result. The NaN values in the columns correspond to key pairs that weren’t present in one of the original dataframes. \n",
    " \n",
    "This code snippet demonstrates the operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.merge(left,right, how='outer', on=['key1','key2'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You have already seen examples of inner joins in Examples 1 and 2, which is the default behaviour of merge(). There are many more possible variations of the __merge()__ and __join()__. \n",
    "\n",
    "And we have already seen the illustration of INNER JOIN in Example 1 and Example 2, which is the default behaviour of __merge()__ function\n",
    "\n",
    "There are many more variations of the merge/join possible, and we encourage you to explore the topic further. Following official documentation provides a well documented and thorough explanation:\n",
    "\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping data sets\n",
    "\n",
    "Now that you have learnt about techniques for combining data sets, what about Python operations to reorganise the information in a single data set? Python has operations for rearranging tabular data, known as reshaping or pivoting operations.\n",
    " \n",
    "For example, hierarchical indexing provides a consistent way to rearrange data in a dataframe. \n",
    " \n",
    "There are two primary functions in hierarchical indexing:\n",
    " \n",
    "- stack(): rotates or pivots data from columns to rows \n",
    "- unstack(): pivots data from rows to columns \n",
    " \n",
    "Here are the syntax for both the functions:\n",
    " \n",
    "<CODE>\n",
    "DataFrame.stack(level=- 1, dropna=True)\n",
    "</CODE>\n",
    " \n",
    "<CODE>\n",
    "DataFrame.unstack(level=- 1, fill_value=None)\n",
    "</CODE>\n",
    " \n",
    "Let's try these operations with some examples. Use these code snippets:\n",
    " \n",
    "First, create a dummy DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create a dummy DataFrame.\n",
    "data = pd.DataFrame(np.arange(6).reshape((2,3)),\n",
    "                    index=pd.Index(['Victoria', 'NSW'], name='state'),\n",
    "                    columns=pd.Index(['one','two','three'], name='number'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the STACK method, we will pivot the columns into rows\n",
    "data_stack = data.stack()\n",
    "data_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that:\n",
    " \n",
    "- the operation converted the columns to row labels\n",
    "- the values now have hierarchical indexing (state and number)\n",
    "- the operation converted the dataframe to a series.\n",
    "\n",
    "You can confirm these changes with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stack.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a hierarchically indexed series, you can rearrange the data back into a dataframe with the __unstack()__ function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_stack.unstack()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the innermost level is unstacked. In our example, it was a number. However, you can unstack a different level by passing a level number or name as a parameter to the unstack method.\n",
    " \n",
    "For example, try this code that unstacks data_stack at the level of state, rather than number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_state = data_stack.unstack('state')\n",
    "data_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation and Transformations\n",
    "\n",
    "So far, we have focused on joining, merging, and rearranging data, but data analytics often requires many other manipulation operations. \n",
    " \n",
    "For example:\n",
    " \n",
    "- bulk transforming records (eg, add missing address information)\n",
    "- detecting and filtering outliers\n",
    "- removing duplicates from a dataset.\n",
    " \n",
    "In this topic, you will explore how Pandas assists with these kinds of tasks. Let’s look at how you would use Python for each of these examples.\n",
    "\n",
    "## Transforming Data using a Function or Mapping\n",
    "\n",
    "\n",
    "For many data sets, you may need to perform some transformations based on values in Pandas Object. \n",
    " \n",
    "Consider this hypothetical scenario:\n",
    " \n",
    "You have a DataFrame consisting of customer names and addresses. The addresses include code and city, but country information is missing. You need to add the country information to this dataframe. Fortunately, you have city-to-country mapping maintained in a dictionary. You want to create an additional column in the dataframe that contains the country values.\n",
    " \n",
    "Let's implement this solution using code. You will use the map method in this particular case. This method is called on a series and you pass a function to it as an argument. The function is applied to each value in the series specified in the map method.\n",
    " \n",
    "Use these code snippets for a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person = pd.DataFrame([\n",
    "    ['Person 1', 'Melbourne', '3024'],\n",
    "    ['Person 2', 'Sydney', '3003'],\n",
    "    ['Person 3', 'Delhi', '100001'],\n",
    "    ['Person 4', 'Kolkata', '700007'],\n",
    "    ['Person 5', 'London', 'QA3023']\n",
    "], columns=['Name','City','Pin'])\n",
    "df_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us create a dictionary for the city and the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mapping = {\n",
    "    \"Melbourne\":\"Australia\",\n",
    "    \"Sydney\":\"Australia\",\n",
    "    \"Delhi\":\"India\",\n",
    "    \"Kolkata\":\"India\",\n",
    "    \"London\":\"United Kingdom\"\n",
    "}\n",
    "dict_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person['Country']= df_person['City'].map(lambda x: dict_mapping[x])\n",
    "df_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe in the result:\n",
    "\n",
    "- the __map__ method is called on the series __df_person['City']__\n",
    "- there is an inline function using lambda notation (covered in Module 2)\n",
    "- this inline function takes __key(x)__ as an input, and returns the value corresponding to this __key(x)__ from the dictionary object __dict_mapping__\n",
    "- the resulting value is stored in a new column, ‘Country’, in the original DataFrame __df_person__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting and Filtering Outliers\n",
    "\n",
    "You learnt about the technique for filtering outliers in Module 3, which discussed boolean indexing. Remember, you can create boolean filters using conditions (eg, data < 3) and then use that boolean filter with indexing on a dataframe and series to extract a subset of data values. Through filtering, you can remove outlier data points from a dataframe or series.\n",
    "\n",
    "As an example, let’s create a dataframe with random numbers. You will extract  particular rows based on a filter applied to values in a particular column.\n",
    "\n",
    "<NOTE>\n",
    "Do note that everytime you run the following codes in your notebook, the output will display a new set of values and not necessarily the ones displayed in the image here.\n",
    "</NOTE>\n",
    "\n",
    "Use these code snippets for a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Random DataFrame\n",
    "data = pd.DataFrame(np.random.randint(5, 1000, size=(1000,4)), columns = ['Col A', 'Col B', 'Col C','Col D'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe the DataFrame, Statistical Summary of the DataFrame\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at Col D, in the output above: \n",
    " \n",
    "- The minimum value is 6. \n",
    "- The maximum value is 999.  \n",
    "- The mean is 511.664. \n",
    " \n",
    "Let's say you want to filter all the records where the value in Col D is less than 400. For this, create a boolean filter on Col D and then apply that filter to the dataframe as an index to get the subset of the data.\n",
    " \n",
    "Try these code snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a filter\n",
    "boolean_filter = data['Col D'] < np.abs(400)\n",
    "boolean_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply this filter on the DataFrame\n",
    "data_filtered = data[boolean_filter]\n",
    "data_filtered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the statistical summary of the filtered dataset, the maximum value in Col D is now 397, which is less than 400. \n",
    " \n",
    "You can apply this filter inline as well, and to all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered_new = data[np.abs(data)<400]\n",
    "data_filtered_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicates\n",
    "\n",
    "There are many real-life situations where you would find duplicate records in a data set such as recurring salary details and email addresses of the same person within the same file or duplicates of addresses for two people of the same household (in some cases, you might need to drop one name and retain the other to avoid duplicate addresses).\n",
    " \n",
    "Pandas has an easy-to-use function, drop_duplicates(), that removes duplicate records from a dataframe.\n",
    " \n",
    "Let's test this function through an example. First, you need to create a test dataframe with some duplicate records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'k1': ['A']*3 + ['B']*4,\n",
    "                    'k2': [1,1,2,2,3,3,4]\n",
    "                   })\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the function to drop the duplicates. \n",
    " \n",
    "__drop_duplicates()__ can work in two ways:\n",
    " \n",
    "- If you don't specify a column, the function operates on all columns collectively (ie, drop rows with exactly the same values in all columns).\n",
    "- If you do specify a column, the function removes duplicates only from that column (ie, keep the first instance in another column).\n",
    "\n",
    "Try this example without a specified column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicate rows\n",
    "df_dedup = df1.drop_duplicates()\n",
    "df_dedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try this example with a specified column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Duplicate Values form Column k1\n",
    "df_dedup_k1= df1.drop_duplicates('k1')\n",
    "df_dedup_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy Mechanics\n",
    "\n",
    "Consider this scenario:\n",
    " \n",
    "You are analysing data for a company that sells computer hardware. The company would like to look at the past year’s sales trends to see which types of hardware were the most profitable, which brands were the most popular, and the average number of sales per region, among other insights. Unfortunately, the company recorded their sales transactions in a rather disorganised way: the transaction records for all offices and regions are stored in a single large file in order of transaction date and time. There are thousands of records and it would be very time consuming to organise them manually. You need to use Python to categorise the transactions, perform necessary calculations for each category, and summarise the results in a useful way.\n",
    " \n",
    "Fortunately, Python has GroupBy operations to help you solve these kinds of business problems.\n",
    " \n",
    "A GroupBy operation is a process with one or more of these steps:\n",
    " \n",
    "- Splitting: the data is separated into groups according to some criteria.\n",
    "- Applying: a function is applied to each group independently.\n",
    "- Combining: the individual results are combined into a data structure.\n",
    " \n",
    "For example, you would use a GroupBy operation to separate some values according to a category, add up the values for each category, then combine the categories for a summary. \n",
    "\n",
    "This diagram illustrates the process:\n",
    " \n",
    "!['Group By Mechanics'](dataframes/Python-GroupBy-demo.png)\n",
    " \n",
    "Out of these steps, splitting is the most straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting an Object into Groups\n",
    "\n",
    "Pandas can split objects along any axes. To create a GroupBy object, call the __groupby()__ method on the dataframe. This will return a dataframe GroupBy object.\n",
    " \n",
    "You can specify splitting criteria or GroupBy mapping in many different ways; for example:\n",
    " \n",
    "a Python function called on each axis label\n",
    "for dataframe objects, a string indicating a column or list of column names for grouping\n",
    "a list or NumPy array of the same length as the selected axis\n",
    "a dictionary or series that provides a label to group name mapping.\n",
    " \n",
    "Collectively, the grouping objects are referred to as ‘keys’. In the example above, 'class' was the key. \n",
    " \n",
    "Try this example that uses the column name as the splitting criterion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_animals = pd.DataFrame([('bird', 'Falconiformes', 389.0),\n",
    "                   ('bird', 'Psittaciformes', 24.0),\n",
    "                   ('mammal', 'Carnivora', 80.2),\n",
    "                   ('mammal', 'Primates', np.nan),\n",
    "                   ('mammal', 'Carnivora', 58)],\n",
    "                  index=['falcon', 'parrot', 'lion', 'monkey', 'leopard'],\n",
    "                  columns=('class', 'order', 'max_speed'))\n",
    "df_animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_class= df_animals.groupby('class')\n",
    "grp_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_class_order = df_animals.groupby(['class', 'order'])\n",
    "grp_class_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see how the __groupby()__ operation returns a dataframeGroupBy object? Importantly, no splitting actually occurs until there is a need. Creating the GroupBy object only verifies that you passed a valid mapping for grouping or splitting.\n",
    " \n",
    "For example, once you call a __.sum()__ method on the two GroupBy objects, the split occurs and returns the summation result. \n",
    " \n",
    "Use these code snippets for a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_class.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_class_order.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Sorting\n",
    "\n",
    "By default, the group keys are sorted during the GroupBy operation. If you don't want group keys to be sorted, pass the parameter sort=false.\n",
    "\n",
    "Try this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'X': ['B', 'B', 'A', 'A','C','C','C'], 'Y': [2, 4, 3, 4,2,5,6]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observer that in the output, Keys are sorted lexicographically\n",
    "df2.groupby(['X']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this statement we are passing sort=False, now group keys will not be sorted\n",
    "df2.groupby(['X'], sort=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Object Attributes\n",
    "\n",
    "If you want to check the created groupings, you can look at the groups attribute of the GroupBy object. \n",
    " \n",
    "This action returns a dictionary with:\n",
    "\n",
    "- keys: computed unique groups\n",
    "- values: corresponding axis labels for each group.\n",
    " \n",
    "Let's check this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'X': ['A', 'B', 'A', 'B'], 'Y': [1, 4, 3, 2]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['X']).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see \n",
    "- the first group key is A, and the corresponding values are indexes 0 and 2,\n",
    "- the second group key is B, and the corresponding values are indexes 1 and 3\n",
    "\n",
    "For the next step, applying, we may need to perform one or a combination of these actions:\n",
    " \n",
    "- Aggregation: compute a summary statistic for each group (eg, sums, means, counts).\n",
    "- Transformation: perform a computation on the data in each group and return a like-indexed object.\n",
    "- Filtration: discard some groups, according to a groupwise computation that evaluates as True or False.\n",
    " \n",
    "You will explore each of these steps in more detail in the remainder of this topic and the subsequent topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation\n",
    "\n",
    "Once you have created the GroupBy object, you have several options to perform computation on the grouped data. These operations are generally referred to as aggregation.\n",
    " \n",
    "There is the __aggregate()__ method, which can be invoked on the GroupBy object. In this method, you can pass the function as a parameter (eg, np.sum). \n",
    "\n",
    "Try this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['X'])\n",
    "grouped.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.aggregate(np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result of aggregation will have the group names (ie, group keys) as the new index along the grouped axis.\n",
    "\n",
    "You can apply various aggregation functions to the GroupBy object. Here are some of them: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various aggregation functions that can be applied to the GroupBy object. Following table list down some of the aggregating functions:\n",
    "\n",
    "|Function|Description|\n",
    "|--------|-----------|\n",
    "|mean()|Compute mean of the values in a group|\n",
    "|sum()|Compute sum of the values in a group|\n",
    "|size()|Compute the group size|\n",
    "|count()|Compute the count of values in a group|\n",
    "|std()|Standard Deviation of the values in a group|\n",
    "|var()|Compute variance of the values in a group|\n",
    "|sem()|Compute the standard of the mean of the groups|\n",
    "|describe()|Generate descriptive statistics of the group|\n",
    "|first()|Compute first of group values|\n",
    "|last()|Compute last of group values|\n",
    "|nth()|Take nth value, of subset if n is a list|\n",
    "|min()|Compute min of the group values|\n",
    "|max()|Compute max of the group values|\n",
    "\n",
    "Let's check some of these aggergation function on the following DataFrame, please see the code snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'X': ['B', 'B', 'A', 'A','C','C','C'], 'Y': [2, 4, 3, 4,2,5,6]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summation Aggegation\n",
    "df.groupby(['X']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean Aggregation\n",
    "df.groupby(['X']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max Aggregation\n",
    "df.groupby(['X']).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply multiple functions at once by using the aggregate function. Try this example where the sum, mean, maximum are calculated together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['X']).aggregate([np.sum,np.mean,np.max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy operations and transformation\n",
    "\n",
    "Aggregation is only one type of group operation. It’s a special case in a more general class of data transformation because it accepts functions that reduce a one-dimensional array to a single value (eg, sum, mean, max).\n",
    " \n",
    "You previously learnt about using the aggregate() method. You can also apply many other transformations to a GroupBy dataframe object, using the transform() method. The transform() method returns an object that is indexed the same (same size) as the object being grouped. \n",
    " \n",
    "This function:\n",
    " \n",
    "- must return a result that’s either the same size as the group size, or can be broadcast to the size of the group chunk\n",
    "- can operate column-by-column on the group chunk\n",
    "- mustn’t perform in-place operations on the group chunk.\n",
    "\n",
    "For example, consider a situation where you need to standardise a set of dates. In this scenario, you cannot rely on simple aggregate functions and would have to rather use the transform() method. You would pass the function for standardising the data within each group as a parameter to the transform method.\n",
    " \n",
    "If you have to standardise a data series, you need to subtract the individual values by the mean of the series, and then divide it by the standard deviation of the data series. So the formulae for standardisation will be:  \n",
    "\n",
    "\n",
    "<code>\n",
    "    x(i) = (x(i) - mean of X Series) / (Standard Deviation of X Series)\n",
    "</code>\n",
    "\n",
    "Let's implement this scenario by simulating a time series of randomly generated dates. Use these code snippets for a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range('10/1/1999', periods=1100)\n",
    "ts = pd.Series(np.random.normal(0.5,2,1100), index)\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculating the Mean over the Rolling Windows\n",
    "ts = ts.rolling(window=100, min_periods=100).mean().dropna()\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s apply a data standardisation transformation to the time series data:\n",
    "\n",
    "- Group the index based on the year (ie, group all the timestamps if they are in the same year).\n",
    "- Transform the values as described above (ie, subtract mean and divide by standard deviation).\n",
    " \n",
    "After this transformation, you would expect that in the transformed data, the mean will be 0 and the standard deviation will be 1. We can check that by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = ts.groupby(lambda x:x.year).transform(lambda x: (x-x.mean())/x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Mean and Standard Deviation of the Transformed Series\n",
    "transformed.groupby(lambda x:x.year).aggregate([np.mean, np.std])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the transformed time series now has a mean of 0 (or floating point number very close to 0) and standard deviation of 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
