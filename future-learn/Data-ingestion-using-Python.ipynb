{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os\n",
    "\n",
    "wd = os.getcwd()\n",
    "wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Reading Data\n",
    "\n",
    "### Data loading and data formats\n",
    "\n",
    "If you recall, in the first module, you learned the amount of data residing in various sources is vast and, also, constantly getting generated in our devices (including cloud). All of this data needs to be accessed at some point in time and it all starts with data ingestion!\n",
    "\n",
    "Data ingestion is a process of reading and loading data into Python from various underlying data sources, such that data can then be processed and transformed as per the requirements of the application. Each kind of data source has their own protocol for transferring data and as an analyst you must understand the difference among them. Most of the time, the loaded data are available to us in the following formats:\n",
    "\n",
    "- Text data (CSV, JSON, Excel, etc)\n",
    "- Web data (HTML, XML)\n",
    "- Databases (SQL and NoSQL Data)\n",
    "- Binary data formats\n",
    "\n",
    "In this module, we will go into the details of some of the functions related to data ingestion for the CSV, JSON, HTML, and SQL data formats. We will also provide references to detailed documentation for other data types and different variations of data ingestion possible.\n",
    "\n",
    "### Reading and writing text data\n",
    "\n",
    "Due to its simple syntax for interacting with files, intuitive data structures, and convenient features like tuple packing and unpacking, Python has become the go-to language for text data. Pandas have several functions for reading tabular data as a DataFrame object. \n",
    "\n",
    "Here are some of these functions:\n",
    "\n",
    "| Function | Description   |\n",
    "|------|------|\n",
    "|   read_csv()  | Load delimited data from a file, URL, or file-like object. “,” – comma is the default delimiter|\n",
    "|  read_table()  | Load delimited data from a file, URL, or file-like object. “\\t” – tab is the default delimiter|\n",
    "|   read_fwf()  | Read data in fixed width column format, that is there is no delimiter|\n",
    "\n",
    "In this course, you will most commonly use the read_csv() and read_table() functions. For the full list of I/O functions available in Pandas you can refer to following link:<br>\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "\n",
    "### CSV functions \n",
    "\n",
    "A CSV (comma separated values) file is a type of plain text file. These csv files, the 'comma separated' files, are how columns and fields are identified in the value.\n",
    "    \n",
    "For example:\n",
    "    \n",
    "ColumnA_header, ColumnB_header, ColumC_header\n",
    "ColumnA_value1, ColumnB_value2, ColumnC_value2\n",
    "ColumnA_value2, ColumB_value2, ColumnC_value3\n",
    "\n",
    "Let us explore the details of the discussed CSV functions used to convert the text data into a DataFrame. \n",
    "If you think about it, you may realise that these functions should have parameters related to the following functionalities and features related to DataFrame:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the details of the discussed CSV functions used to convert the text data into a DataFrame. \n",
    "\n",
    "If you think about it, you may realise that these functions should have parameters related to the following functionalities and features related to DataFrame:\n",
    "\n",
    "#### Indexing options \n",
    "\n",
    "Column names need to be read from the file, or the user, or not at all, and use the default column name conventions\n",
    "\n",
    "Row labels need to be considered as a particular data column in the file as the row label, or use the default row indexing\n",
    "\n",
    "For example: \n",
    "Naming individual columns and rows with unique names so that they are identified when called\n",
    "\n",
    "#### Type inferences and data conversion options\n",
    "\n",
    "Value conversion options as defined by the user.\n",
    "\n",
    "Custom input data to deal with missing value markers.\n",
    "\n",
    "For example:\n",
    "Converting a particular value in a cell of the DataFrame; this also included replacing with a new value or inputting the default NaN for missing values.\n",
    "\n",
    "#### DateTime parsing-related options\n",
    "\n",
    "Capability to combine the date and time information spread across multiple columns in the input data and merging the combined data into a single column in the result.\n",
    "\n",
    "For example:\n",
    "Combining month, day, and year to produce a full date.\n",
    "\n",
    "#### Iteration-related options\n",
    "\n",
    "Iterate over smaller chunks of data in the case of large files.\n",
    "\n",
    "For example:\n",
    "Repeating the same customer ID for a particular name anywhere in the file.\n",
    "\n",
    "#### Data Issue-related options\n",
    "\n",
    "Options to deal with data nuances\n",
    "\n",
    "For example: \n",
    "Skipping rows or a footer, comments, or others such as numeric data with commas used in the representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating CSV functions\n",
    "\n",
    "Now, let’s evaluate these functions and various options. There are different sample CSV files that are stored in the module directory (if you don’t have one created, create one for storing all the files you just downloaded under datasets in the start of this topic), and we will be using these files to perform the read operations and numerous variations of it.\n",
    "\n",
    "You can see that there are comma-separated values in the file. We’ll show you how to use the function read_csv() and also check the various parameters available for this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Read Operation of the csv File\n",
    "\n",
    "Make sure you have the file named sample_data.txt downloaded on your machine. Next, enter the code snippet that demonstrates the use of the read_csv function:\n",
    "\n",
    "Following code snippet demonstrates the use of the __read_csv__ function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the sample data from the dataset folder. Filename is stored in the variable filename\n",
    "filename = \"./dataset/sample_data.txt\"\n",
    "df= pd.read_csv(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the following can be observed:\n",
    "\n",
    "- The first row of the file has been considered as the name of the columns by default.\n",
    "- The default row labels (row indexes) have been considered (look at the values 0,1,2,3,4).\n",
    "- Parsing of the values has happened automatically (i.e., we didn't have to specify the ’,’ as the delimiter explicitly).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the Column Names\n",
    "\n",
    "Tables in excel generally come with the header rows that contain the information that either identifies the content of a particular column or the number of the column. \n",
    "\n",
    "There are scenarios where you may have to explicitly specify whether the header row in a table exists or not. The parameter header of the function controls this behaviour. So if you pass the value header=None, it will not consider the first row as the header and instead as a data record. In this case, column names for the DataFrame will automatically generate.\n",
    " \n",
    "Make sure you have the file named sample_data_noheader.txt downloaded on your machine.\n",
    "\n",
    "Next, we will now read this file and instruct the read_csv function to consider the first row as the data record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./dataset/sample_data_noheader.txt\"\n",
    "df= pd.read_csv(filename, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular scenario, there could be a requirement to explicitly provide the column names instead of relying on the auto indexing for column names.\n",
    "\n",
    "In that case we use the parameter __names__ and pass the list of column names to the read_csv() function. When we pass the __names=[list of column names]__, we dont have to pass the parameter __header=None__ to the read_csv function.\n",
    "\n",
    "See below code snippet for the demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['a','b','c','d','comments']\n",
    "df=pd.read_csv(filename, names=names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the Row Labels / Row Index from a Column in the data file\n",
    "\n",
    "Let us say you now have your columns labeled with individual header rows such as mango, apple, and grape. You might then decide to explicitly name that row as fruits. Python comes handy in such scenarios where you would like to explicitly specify the row labels (row indexes) instead of using the default indexes.\n",
    "\n",
    "In the current example, assume that we want the comment section to be the row label of the DataFrame. This can be achieved by using the __index_col__ parameter of the function, and specifying the name of the column to be used as the row label.\n",
    "\n",
    "See the code snippet below for the demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(filename, names=names, index_col='comments')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['comment1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the following can be observed:\n",
    "\n",
    "the comment column from the input data file has now been used to specify the row labels of the DataFrame\n",
    "\n",
    "you can access the first row of the DataFrame using the key __\"comment1\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Indexing\n",
    "\n",
    "Let us say you want to include a hierarchical indexing functionality. Instead of one column (from the previous example, such as Fruits) considered as an index, you have a hierarchy of columns considered as indexes (such as fruit varieties Tropical and Exotic). These indexes can have the same values such as mango, apple, and grape; but now they are specific to the index names and can either be a tropical mango, apple, and grape or an exoctic mango, apple, and grape.\n",
    "\n",
    "Make sure you have the file named sample_data_hierarchy.txt downloaded on your machine.\n",
    "\n",
    "In this particular case first two columns together can be considered as the row index. For such a scenario we would have to pass the list of columns to be considered as hierarchical index to the __read_csv__ function, using the __index_col__ parameter.\n",
    "\n",
    "See the code snippet below for demonstration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./dataset/sample_data_hierarchy.txt\"\n",
    "names=['I1','I2','col1','col2','col3','col4','comments']\n",
    "df= pd.read_csv(filename, names=names, index_col=['I1','I2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['A',1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the following can be observed:\n",
    " \n",
    "- we have read the data file, specified the column names bypassing the list of column names to the parameter __names__\n",
    "- we have also specified the hierarchical indexing to be used by passing the list of column names to be considered as indexes to the parameter __index_col__\n",
    "- we are accessing the first row of the DataFrame using the hierarchical index 'A',1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the Seperator / Delimiter explicitly\n",
    "\n",
    "You were introduced to csv as comma separated values files, where the default delimiter was a comma (,). But, what if that default delimiter would not work? In such cases, you may have to specify the separator / delimiter explicitly when reading the data from the file. \n",
    "\n",
    "For example:\n",
    "\n",
    "If white space is used as the separator instead of the default value comma ',', you might need to write a separate code to call out the delimiter explicitly. \n",
    "\n",
    "In this case, you can specify the separator to be used by using the parameter __sep__ and passing the value of the separator. \n",
    "\n",
    "Consider the file named sample_data_noheader_space.txt (make sure you have this downloaded on your machine) where white space is used as the separator / delimiter. To read this file correctly in the DataFrame, we will use the parameter __sep__\n",
    "\n",
    "The code snippet demonstrates when we have not used the sep parameter, and all the values in the first column of the DataFrame have been read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./dataset/sample_data_noheader_space.txt\"\n",
    "names=['a','b','c','d','comments']\n",
    "df= pd.read_csv(filename, header=None, names=names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see when we have not used the parameter __sep__ all the values have been read in the first column of the DataFrame. Let's now specify the __sep__ parameter and read the data correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(filename, header=None, names=names, sep=\" \")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above the data has now been read correctly into the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping the specified Rows while reading data from a file\n",
    "\n",
    "There could be scenario's where specific rows need to be ignored while reading the data from a file.\n",
    "\n",
    "For example: \n",
    "\n",
    "- when there are fixed line comments at the top, or\n",
    "- when you have comments at the fixed line numbers in the file, or\n",
    "- say the footer line in the text file.\n",
    "\n",
    "In this case, we will use the __skiprows__ parameter of the __read_csv__ function. We have to pass the index number of the row to this parameter. Whilst, in case we want to ignore multiple rows, we must be able to pass the list of row indexes to this parameter.\n",
    "\n",
    "In this particular case we will ignore the 1st, 2nd, 3rd and 6th row by passing the indexes 0,1,2 and 5 respectively.\n",
    "See the code snippet below for the demonstration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./dataset/sample_data_comments.txt\"\n",
    "names=['a','b','c','d','comments']\n",
    "df= pd.read_csv(filename, header=None, names=names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above if we dont specify the __skiprows__ parameter, then the DataFrame has not been parsed properly. The first 3 comment rows have been read and depending on whethere there is , in the comment the value has been parsed into either the first column or more.\n",
    "\n",
    "We will now perform this read operation by correctly specifying the __skiprows__ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(filename, header=None, names=names, skiprows=[0,1,2,5])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see now, the DataFrame has been correctly populated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping the Footer while reading a Text File\n",
    "\n",
    "Similarly, a text file may contain elements such as a footer that you may want to ignore as a row while reading the file because it might be irrelevant to the analysis you are conducting. In such cases, you will use the parameter __skipfooter__ and pass an integer value equal to the number of lines to ignore while reading the file. \n",
    "\n",
    "Make sure you have the file named sample_data_footer.txt downloaded on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./dataset/sample_data_footer.txt\"\n",
    "names=['a','b','c','d','comments']\n",
    "df= pd.read_csv(filename, header=None, names=names, skipfooter=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data while reading data from a file\n",
    "\n",
    "In the examples shown, you may have observed that when data is missing, the default behaviour is to replace the missing value with NaN. Handling missing values is an integral part of the file-parsing process, with some subtle nuances:\n",
    "\n",
    "Explicitly specifying the input values to be considered as missing values while reading the file.\n",
    "These are scenarios when a specific naming convention may denote the missing values in the input file. \n",
    "\n",
    "For example:\n",
    "- Some data might have ‘Missing’ to specify the missing data values\n",
    "- Some might even have ‘99999999’ to specify missing reading in case of numeric values\n",
    "- Some might even have a combination of both ‘Missing’ and ‘99999999’ for different data columns in the file\n",
    "\n",
    "If you consider the next file example, some rows have missing values. Look for no value between two commas. Also, there are some rows where it is explicitly noted as ‘Missing’ (the notation could be anything; eg, NA instead of missing).\n",
    " \n",
    "Make sure you have the file named sample_data_missingvalues.txt downloaded on your machine. \n",
    "The code snippet demonstrates reading this file first without explicitly specifying that ‘Missing’ be considered as a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default read operation, without specifying the explicity missing value notations\n",
    "filename = \"./dataset/sample_data_missingvalues.txt\"\n",
    "names=['a','b','c','d','comments','Value']\n",
    "df= pd.read_csv(filename, header=None, names=names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s read the same file by specifying the missing values using the __na_values__ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read operation by specifying a missing value notation\n",
    "df= pd.read_csv(filename, header=None, names=names, na_values=\"Missing\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the following can be observed:\n",
    "\n",
    "- Row index 5, and columns b and comments – the string ‘Missing’ has been inferred as missing value NaN.\n",
    "- Row index 6 and column b and c – the string ‘Missing’ has been inferred as missing value NaN.\n",
    "\n",
    "We can extend this scenario and set the following requirements:\n",
    "\n",
    "- Columns a,b,c,d and comments – string ‘Missing’ has to be treated as missing values.\n",
    "- Column ‘Value’ – value 999999 has to be treated as a missing value.\n",
    "\n",
    "This can be achieved by passing the dictionary object to na_values where:\n",
    "\n",
    "- key – the name of columns.\n",
    "- value – the list of values to be considered as the missing value for those columns.\n",
    "\n",
    "The code snippets demonstrate how:\n",
    "\n",
    "- The values 'foo' and 'Missing' will be considered as missing values for columns a,b,c,d and comments.\n",
    "- The values -1 and 999999 will be considered as the missing value for the column Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_missing = {\n",
    "    'a':['foo','Missing'],\n",
    "    'b':['foo','Missing'],\n",
    "    'c':['foo','Missing'],\n",
    "    'd':['foo','Missing'],\n",
    "    'comments':['foo','Missing'],\n",
    "    'Value':[999999, -1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(filename, header=None, names=names, na_values=dict_missing)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text File in Pieces / Small Chunks\n",
    "\n",
    "When processing huge files, you may want to:\n",
    "\n",
    "- only read a small portion of the file to figure out the right set of arguments to be used\n",
    "- read the file in smaller chunks and iterate over it sequentially.\n",
    "\n",
    "There are two parameters we can use in this scenario:\n",
    "- nrows –  specifies the number of rows to be read.\n",
    "- chunksize – specifies the number of rows to be read as one chunk. \n",
    "\n",
    "In this particular case, the read_csv will return a File Iterator instead of a DataFrame. We can use this iterator to create the resulting DataFrame or perform quantitative calculations.\n",
    "\n",
    "Make sure you have the file named sample_data_large.csv downloaded on your machine. \n",
    "\n",
    "The code snippet demonstrates using __nrows__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the smaller sample of the file by specifying the nrow parameter\n",
    "filename = \"./dataset/sample_data_large.csv\"\n",
    "df= pd.read_csv(filename, nrows=10)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see only 10 top rows have been read in this particular case.\n",
    "\n",
    "Let's now explore the use of __chunksize__ and __File Iterator__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the big file in one go to check whether it has reached the limit or not,\n",
    "#and verify the row count\n",
    "filename = \"./dataset/sample_data_large.csv\"\n",
    "df= pd.read_csv(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the memory footprint of the DataFrame that we have just read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The memory footprint of the DataFrame is: %f MB\"%(df.memory_usage().sum()/(1024 * 1024)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the requirement is to calculate the sum of the values in the ‘Values’ column. This can be achieved without creating a DataFrame by using the File Iterator and performing the summation for the specific columns.\n",
    "\n",
    "This would have a significantly smaller memory footprint and would result in code that performs better. We can access the File Iterator by:\n",
    "\n",
    "- using the chunksize parameter and specifying the integer value\n",
    "- specifying the iterator parameter and passing the boolean value True.\n",
    "\n",
    "If you calculate this sum using DataFrame as well as the iterator, conducting memory profiling for both operations, you can expect that: \n",
    "\n",
    "- summation using DataFrame will require a larger memory footprint but less execution time\n",
    "- summation using Iterator will have lesser memory footprint (but more execution time because of multiple data movements to memory for calculation.\n",
    "\n",
    "The code snippet demonstrates using Python's memory and execution time profile to check these values. \n",
    "\n",
    "If you don't have the module install it using the following command in your terminal/shell:\n",
    "\n",
    "conda install -c anaconda memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import memory_profiler, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory and Time Profiling with the DataFrame Operation\n",
    "if __name__ == '__main__':\n",
    "    m1 = memory_profiler.memory_usage()\n",
    "    t1 = time.process_time()\n",
    "#Print the sum     \n",
    "    filename = \"./dataset/sample_data_large.csv\"\n",
    "    df= pd.read_csv(filename)\n",
    "    total = df['Values'].sum()\n",
    "    print(\"Total Values: %d \"%total)\n",
    "\n",
    "    t2 = time.process_time()\n",
    "    m2 = memory_profiler.memory_usage()\n",
    "    time_diff = t2 - t1\n",
    "    mem_diff = m2[0] - m1[0]\n",
    "    print(f\"It took {time_diff} Secs and {mem_diff} Mb to execute this method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory and Time Profiling with the File Iterator Operation\n",
    "if __name__ == '__main__':\n",
    "    m1 = memory_profiler.memory_usage()\n",
    "    t1 = time.process_time()\n",
    "#Print the sum     \n",
    "    filename = \"./dataset/sample_data_large.csv\"\n",
    "    iter = pd.read_csv(filename,chunksize=1000)\n",
    "    total_value = 0\n",
    "    for record in iter:\n",
    "        total_value+=record['Values'].sum()\n",
    "    print(total_value)\n",
    "\n",
    "    t2 = time.process_time()\n",
    "    m2 = memory_profiler.memory_usage()\n",
    "    time_diff = t2 - t1\n",
    "    mem_diff = m2[0] - m1[0]\n",
    "    print(f\"It took {time_diff} Secs and {mem_diff} Mb to execute this method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory footprint is significantly less when using File Iterator.\n",
    "\n",
    "\n",
    "__Note:__ We have used read_csv() in our demonstration, but we can also use the read_table() function. The main difference between the two functions is the default value of delimiter / separator considered by the two functions. \n",
    "read_csv function considers a comma ',' as the default delimiter. \n",
    "read_table function considers tab '\\t' as the default delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed information on the numerous parameters that can be specified for the read_csv() function, refer to the following link: \n",
    "\n",
    "Go to:  Pandas 1.1.3 documentation [2]\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data in JSON Format\n",
    "\n",
    "JSON (Javascript Object Notation) is now a widely used data format in web applications. Almost all of the current APIs transfer data using JSON format. It is a flexible data format in comparison to the tabular data formats like CSVs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Basic Read Operation of the JSON File\n",
    "\n",
    "Pandas has the function __read_json()__ to read JSON formatted data. In its simplest form, you need to specify the location of the JSON data file, and it will be read into a DataFrame.\n",
    "\n",
    "Consider the below sample file having a JSON data format. Make sure you have the file sample_data.json downloaded on your machine.\n",
    "\n",
    "You may realise that this data representation is very similar to Python's dictionary notation. In this particular case, the default behaviour would be to consider:\n",
    "\n",
    "- the subkeys (0,1,2, etc) as the row labels.\n",
    "- the top-level keys (Product and Price) as the column labels.\n",
    "\n",
    "The code snippet demonstrates this behaviour:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./dataset/sample_data.json\"\n",
    "df= pd.read_json(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how the data is formatted in JSON, it would have an impact on how the DataFrame is created. \n",
    "\n",
    "By default:\n",
    "\n",
    "- top-level key is considered the Column Name (0,1,2, etc)\n",
    "- key at nested level 1 is considered the Row Label (Price, Product).\n",
    "\n",
    "Consider the following JSON formatted file. Make sure you have the file sample_data_index.json downloaded on your machine:\n",
    "\n",
    "As highlighted, this time ‘0,1,2’ will be the column labels and ‘Price and Product’ will become the row labels.\n",
    " \n",
    "The code snippet demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./dataset/sample_data_index.json\"\n",
    "df= pd.read_json(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may need to control the behaviour in terms of how the DataFrame is created. Say for e.g., using the same format of the input JSON data you want that Price and Product be the Column Names, and 0,1,2..etc., be the row labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Orientation in JSON files\n",
    "\n",
    "Data orientation behaviour can be controlled using the parameter orient. This parameter has following possible values:\n",
    "\n",
    "- 'split' – dict like {index -> [index], columns -> [columns], data -> [values]}\n",
    "- 'records' – list like [{column -> value}, ... , {column -> value}]\n",
    "- 'index' – dict like {index -> {column -> value}}\n",
    "- 'columns' – dict like {column -> {index -> value}}\n",
    "- 'values' – only the values array\n",
    "\n",
    "As you can infer from the examples, the default expected orientation is Columns Orientation.\n",
    "\n",
    "#### INDEX formatted JSON Data\n",
    "\n",
    "In the example, the JSON data is formatted in the index format (ie {row#:{Column: Value}} format) so if we specify the orient=index, we will get the DataFrame in the desired format.\n",
    " \n",
    "Consider the following JSON formatted file we had used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_json(filename, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPLIT formatted JSON Data\n",
    "\n",
    "Consider the following JSON formatted file - JSON data is formatted as below i.e. {index:{values}, column1:{values}, columns2:{values}. Make sure you have the file sample_data_split.json downloaded on your machine:\n",
    "\n",
    "You will use the __orient=split__ specification to create the DataFrame properly.\n",
    "\n",
    "The code snippet demonstrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./dataset/sample_data_split.json\"\n",
    "df= pd.read_json(filename, orient='split')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerous variations and customisations may be needed while using the read_json function, depending on the structure of the data in the JSON formatted file. In this section, we have only covered the basics, and for detailed reference of input / output related JSON, please refer to the following link: \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/version/0.23.4/io.html#json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading HTML and XML Data: Web Scrapping\n",
    "\n",
    "At many times, when websites don't provide downloadable, machine-readable data formats like JSON, XML, and so on, the data might be represented as HTML formatted tables.\n",
    " \n",
    "If this happens, you may need a way to read the HTML tables directly. Pandas have a top-level __read_html()__ function that can accept an HTML string / file / URL and will parse the HTML table into the list of the Pandas DataFrame.\n",
    " \n",
    "### Basic read of HTML files / URLs\n",
    " \n",
    "The __read_html()__ function can be used to read HTML formatted data. Let's understand this behaviour using an example list from the Federal Deposit Insurance Corporation (FDIC) in the USA. The FDIC is often appointed as a receiver for failed banks and the list we’re using includes information on banks that have failed since 1 October 2000.  \n",
    "    \n",
    "\n",
    "We will extract the data from the following URL:<br>\n",
    "__'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\n",
    "dfs = pd.read_html(url)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the following can be observed:\n",
    " \n",
    "- this call has extracted all the 561 data records from the HTML table on the web pages\n",
    "- the output data is in the List Format, and not the DataFrame. This way, if there are multiple HTML tables on the webpage, we would get the list of various data frames.\n",
    "\n",
    "\n",
    "Next, let's check the __type__ of variable dfs – this should be a list. If we access the __first element__ of the list, this should return a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dfs[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try inputting the code shown below and check what could be the possible return? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen in read_csv and read_json functions that there are numerous configuration parameters that control how the data is read, and the same is true with the read_html() function as well.\n",
    "\n",
    "We will explore some of the standard specifications next:\n",
    " \n",
    "- specifying the row index\n",
    "- specifying the header value\n",
    "- specifying the number of rows to be skipped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the Row Label / Row Index\n",
    "\n",
    "This can be done by using the __index_col__ parameter, and specifying the column name or index of the columns to be used as the Row Label. In our example we will use first column as the Row Labels, and hence we will pass the value __0__ to this parameter\n",
    "\n",
    "See the code snippet below for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url='https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\n",
    "dfs = pd.read_html(url,index_col=0)\n",
    "df=dfs[0] #first element of list is a dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['The First State Bank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\n",
    "dfs = pd.read_html(url,skiprows=10)\n",
    "df=dfs[0] #first element of list is a dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet shows how we can access the records using the row label (bank names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/'\n",
    "#dfs1=pd.read_html(url,attrs={'id,table'}) #this is commented out\n",
    "dfs2=pd.read_html(url,attrs={'class':'dataTable text-light dataTables-sidebar overflow-x-auto'})\n",
    "#print(np.array_equal(dfs[0],dfs2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs2[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Null Values \n",
    "\n",
    "Using the parameter na_values, we can pass the list of data values considered as null.\n",
    "\n",
    "The code snippet shows passing the value  __'No Acquirer'__ as the null value to the read_html function, which will then replace these values with NaN when stored in the DataFrame:\n",
    "\n",
    "First specify the null values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying Null Values\n",
    "dfs=pd.read_html(url,attrs={'class':'dataTable text-light dataTables-sidebar overflow-x-auto'}, na_values=[\"No Acquirer\"], index_col=0)\n",
    "df=dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['Dollar Savings Bank', 'Bank of Alamo']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Web Scrapping is a very wide topic, and you can spend days and months just focusing on Web Scrapping. In this section we have just touched on the very basics of the scraping HTML Data.\n",
    "\n",
    "Python ecosystem provides variety of libraries that can be used for Web Scrapping. For e.g.\n",
    "1.  __lxml__ -> provides both HTML and XML Scrapping and Data Parsing capabilities\n",
    "2. __BeautifulSoup4__ -> is another library which is extensively used in Web Scrapping\n",
    "3. __HTML5LIB__ -> is another library that deals with the HTML5 data foramts.\n",
    "\n",
    "If you would like to explore this area further and get into the nitigritties of the topic, there are various books and references available. Some of them are highlighted below:\n",
    "\n",
    "- https://www.oreilly.com/library/view/web-scraping-with/9781491985564/\n",
    "- https://realpython.com/beautiful-soup-web-scraper-python/\n",
    "- https://towardsdatascience.com/how-to-web-scrape-with-python-in-4-minutes-bc49186a8460\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading SQL data from database servers\n",
    "\n",
    "In real-life applications, most of the time data is stored in the SQL-based relational databases such as MySQL, SQL Server, PostgreSQL, and so on, and various non-SQL databases like NoSQL, MongoDB, and so on. \n",
    " \n",
    "The database in use is dependent on various architectural factors, and Python provides the mechanisms to read the data from the databases directly. This loading of data from SQL to Pandas is relatively straightforward once the connection to the database is established because both the SQL tables and DataFrame have similar constructs; that is, row labels and column labels.\n",
    "\n",
    "In Module 6, we go into the details of database designs, data definition, and data manipulation language, as well as SQL operations, to read the databases and perform various analytics activities.\n",
    "For this section, we will leverage an in-memory SQLite database using Python's built-in SQLite3 server. The focus of this session will be to read the data from the existing tables only.\n",
    "\n",
    "Run the following code by creating a dummy database and data in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create Database and Dummy Data\n",
    "import sqlite3\n",
    "\n",
    "query=\"\"\"\n",
    "CREATE TABLE PERSON(\n",
    "FirstName VARCHAR(20),\n",
    "LastName VARCHAR(20),\n",
    "Age INTEGER,\n",
    "City VARCHAR(20),\n",
    "Country VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "con = sqlite3.connect(':memory:')\n",
    "con.execute(query)\n",
    "con.commit()\n",
    "\n",
    "data = [('John', 'Adams', 34, 'New York', 'USA'),\n",
    "('John', 'Mathews', 34, 'Berlin', 'Germany'),\n",
    "('John', 'Abraham', 44, 'Paris', 'France'),\n",
    "('Manoj', 'Sharma', 31, 'Delhi', 'India'),\n",
    "('Pramod', 'Saini', 34, 'New York', 'USA'),\n",
    "('Mark', 'Rolton', 42, 'Brisbane', 'Australia')]\n",
    "\n",
    "stmt = \"INSERT into Person VALUES(?,?,?,?,?)\"\n",
    "con.executemany(stmt, data)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data from SQL Database\n",
    "\n",
    "We have created a table named __Person__ in SQLite3, an in-memory database available with Python 3, and we will explore how to access the data from this database table.\n",
    " \n",
    "In general, execute the following steps when you want to connect and access a database from any database via any programming language:\n",
    "\n",
    "- Create a connection object with the database, also known as a connection string.\n",
    "- Draft the SQL Query execution on that connection.\n",
    "- Using the Execute Method provided by that Programming Language, run the query over the connection created.\n",
    " \n",
    "For Python, we have a __read_sql()__ method provided by Pandas in the pandas.io.sql module, and we will leverage that to read the data from the database table Person.\n",
    " \n",
    "See the code snippet below where we perform the above three activities, and read the data from the table named __Person__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.io.sql as sql\n",
    "import sqlite3\n",
    "\n",
    "#Step1: Create Connetion Object or use Previously created connectiom obect\n",
    "#con = sqlite3.connect(':memory:')\n",
    "\n",
    "#Step 2: Create Query to be executed\n",
    "query = 'SELECT * from PERSON'\n",
    "\n",
    "#Step 3: Execute the Query using read_sql function, and passing the query \n",
    "#and connection objects\n",
    "df = pd.read_sql(query, con)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing DataFrame data to Files\n",
    "\n",
    "So far, we have used various functions to read the data from files into Pandas DataFrame. There may be scenarios where we may have to write the data back into files. Pandas provide the functions to write the data back into files. In this section, we will look at the following two functions:\n",
    "\n",
    "- to_csv() -> this method can create a CSV file based on the data in the DataFrame.\n",
    "- to_json() -> this method can create a JSON file based on the data in the DataFrame.\n",
    "\n",
    "### Writing a CSV file\n",
    "\n",
    "The code snippet shows where we will create a CSV file using the function read_csv() function on a DataFrame.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a csv file with the above data\n",
    "filename = \"dataset/file_csv_1.csv\"\n",
    "df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the file has been created at the specified location i.e. __\"dataset/\"__. See the screenshot below:<br>\n",
    "![\"CSV File Created\"](file_csv_1.png)\n",
    "<br> You can also see the data in the file, see the screenshot below:<br>\n",
    "![\"CSV_File_Data\"](file_csv_1_data.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a JSON File\n",
    "\n",
    "See the below code snippet below where we will create a JSON file using the function to_json() on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"dataset/file_json_1.json\"\n",
    "df.to_json(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the file has been created at the specified location i.e. __\"dataset/\"__. See the screenshot below:<br>\n",
    "![\"CSV File Created\"](file_json_1.png)\n",
    "<br> You can also see the data in the file, see the screenshot below:<br>\n",
    "![\"CSV_File_Data\"](file_json_1_data.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the default data orientation of  'Column' is used to create the JSON file.\n",
    "\n",
    "You can control the data orientation in the __to_json()__ function by passing an orient parameter. \n",
    "\n",
    "The code snippet shows where we will pass the __orient='split'__ parameter, and you can see the resulting JSON file has a different data orientation now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"dataset/file_json_split.json\"\n",
    "df.to_json(filename, orient='split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the file has been created at the specified location i.e. __\"dataset/\"__. See the screenshot below:<br>\n",
    "![\"CSV File Created\"](file_json_split.png)\n",
    "<br> You can also see the data in the file, see the screenshot below:<br>\n",
    "![\"CSV_File_Data\"](file_json_split_data.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
